import torch
from fla.layers.kda import KimiDeltaAttention

def test_raw_kda():
    print("ğŸ§ª æµ‹è¯•çº¯å‡€ KDA å±‚...")
    device = "cuda"
    dtype = torch.bfloat16
    
    B, L, H, D = 2, 128, 12, 128 # æ¨¡æ‹Ÿä½ çš„å‚æ•°
    hidden_size = H * D
    
    # å®ä¾‹åŒ–å•å±‚
    layer = KimiDeltaAttention(
        mode='chunk',
        hidden_size=hidden_size,
        head_dim=D,
        num_heads=H,
        num_v_heads=H, # GQA
        chunk_size=64,
        use_short_conv=True
    ).to(device).to(dtype)
    
    x = torch.randn(B, L, hidden_size, device=device, dtype=dtype).contiguous()
    
    try:
        y, _, _ = layer(x)
        print("âœ… KDA å•å±‚æ­£å‘é€šè¿‡")
        y.sum().backward()
        print("âœ… KDA å•å±‚åå‘é€šè¿‡")
    except Exception as e:
        print(f"âŒ KDA å•å±‚å´©æºƒ: {e}")
        print("ç»“è®ºï¼šè¿™æ˜¯ fla åº“çš„é—®é¢˜ï¼Œä¸æ˜¯ä½ ä»£ç çš„é—®é¢˜ã€‚")

if __name__ == "__main__":
    test_raw_kda()