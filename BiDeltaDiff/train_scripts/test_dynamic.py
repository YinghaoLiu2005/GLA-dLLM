import torch
from BiDeltaDiff.models import BiDeltaDiffForCausalLM, BiDeltaDiffConfig
CUDA_VISIBLE_DIVICES=1
device ="cpu"
def test_model_sanity():
    print("ğŸš€ å¼€å§‹å†’çƒŸæµ‹è¯•...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # 1. ä½¿ç”¨ä½ åˆšæ‰ä¿®æ”¹å¥½çš„ Config
    config = BiDeltaDiffConfig(
        vocab_size=151936,
        hidden_size=1536,
        intermediate_size=8960,
        num_hidden_layers=2, # æµ‹è¯•åªå¼€2å±‚ï¼Œçœæ—¶é—´
        num_heads=12,
        head_dim=128,
        chunk_size=64,
        is_bidirectional=True,
        fuse_norm=True,
        fuse_swiglu=True
    )

    # 2. å®ä¾‹åŒ–æ¨¡å‹
    model = BiDeltaDiffForCausalLM(config).to(device).to(torch.bfloat16)
    model.train() # å¼€å¯è®­ç»ƒæ¨¡å¼ (è§¦å‘ Fast-dLLM çš„æ‰©æ•£ Mask é€»è¾‘)
    print("âœ… æ¨¡å‹å®ä¾‹åŒ–æˆåŠŸ")

    # 3. æ„é€  Dummy Input
    # æ³¨æ„ï¼šé•¿åº¦æœ€å¥½æ˜¯ chunk_size (64) çš„å€æ•°ï¼Œæ¯”å¦‚ 128
    B, L = 2, 128 
    input_ids = torch.randint(0, 10000, (B, L)).to(device)
    labels = input_ids.clone() # éšä¾¿é€ ç‚¹ label

    # 4. å‰å‘ä¼ æ’­æµ‹è¯• (Forward)
    try:
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œ Forward...")
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        logits = outputs.logits
        print(f"âœ… Forward æˆåŠŸ! Loss: {loss.item()}, Logits Shape: {logits.shape}")
    except Exception as e:
        print(f"âŒ Forward å¤±è´¥: {e}")
        return

    # 5. åå‘ä¼ æ’­æµ‹è¯• (Backward) - æ£€æŸ¥æ¢¯åº¦æµæ˜¯å¦æ–­è£‚
    try:
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œ Backward...")
        loss.backward()
        print("âœ… Backward æˆåŠŸ! æ¢¯åº¦è®¡ç®—æ­£å¸¸ã€‚")
    except Exception as e:
        print(f"âŒ Backward å¤±è´¥ (å¯èƒ½æ˜¯ inplaceæ“ä½œæˆ–è®¡ç®—å›¾æ–­è£‚): {e}")
        return

    print("ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ¨¡å‹ç»“æ„åœ¨æ•°å­¦å’Œå·¥ç¨‹ä¸Šéƒ½æ˜¯é€šçš„ï¼å¯ä»¥å¼€å§‹åŠ è½½æƒé‡è®­ç»ƒäº†ï¼")

if __name__ == "__main__":
    test_model_sanity()